Build and deploy a fully private Al analyst completely self-hosted, no third-party APIs, and compliant with strict legal data policies.

A full blown internal system that is pretty much an analyst, trained to process subject matter, answer complex questions, and summarize docs but with zero exposure to OpenAl or Anthropic. 
With control, privacy and automation that can run on a laptop.

Smaller, CPU-Friendly AI Setup Tech stack 

1. Language Model (LLM) models that run efficiently on CPUs.
	Mistral 7B,
	Phi-2,
	Gemma 2Bâ€”smaller 
  Use GGUF quantized models to reduce RAM usage (e.g., use llama.cpp to run them locally).

2. Vector Database (For Retrieval)
	ChromaDB, allows for continual updates, not as fast
	FAISS, extremely fast, does not allow for updates

3. RAG Pipeline (Retrieval-Augmented Generation)
	LlamaIndex to abstract the interfaces and ease developement, but 
		doesn't support adding documents with metadata directly (requires backend interface)
		requires powerful LLM to parse JSON in simple pipelines due to hard-coded prompts
	LangChain to fetch relevant documents before passing them to the model.
	Haystack - contains many wrappers like LLamaIndex. Worried about long-term adoptability.
